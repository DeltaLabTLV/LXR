{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18831b05",
   "metadata": {},
   "source": [
    "### This notebook produces the metrics for a specific recommendation system and dataset.\n",
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160299a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import importlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59163763",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, Yahoo, Pinterest\n",
    "recommender_name = \"MLP\" ### Can be MLP, VAE\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir.parent, DP_DIR)\n",
    "checkpoints_path = Path(export_dir.parent, \"checkpoints\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56467a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\"\n",
    "}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155\n",
    "}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362\n",
    "}\n",
    "\n",
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP1_ML1M_0.0076_256_7.pt\"),\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_0.0002_32_12.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\")\n",
    "}\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): [512,128],\n",
    "    (\"ML1M\",\"MLP\"): 32,\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): [512,128],\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "\n",
    "    (\"Pinterest\",\"VAE\"): [512,128],\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "\n",
    "}\n",
    "\n",
    "LXR_checkpoint_dict = {\n",
    "    (\"ML1M\",\"VAE\"): ('LXR_ML1M_VAE_26_38_128_3.185652725834087_1.420642300151426.pt',128),\n",
    "    (\"ML1M\",\"MLP\"): ('LXR_ML1M_MLP_12_39_64_11.59908096547193_0.1414854294885049.pt',64),\n",
    "\n",
    "    (\"Yahoo\",\"VAE\"): ('LXR_Yahoo_VAE_neg-1.5pos_combined_19_26_128_18.958765029913238_4.92235962483309.pt',128),\n",
    "    (\"Yahoo\",\"MLP\"):('LXR_Yahoo_MLP_neg-pos_combined_last_29_37_128_12.40692505393434_0.19367009952856118.pt',128),\n",
    "\n",
    "    (\"Pinterest\",\"VAE\"): ('LXR_Pinterest_VAE_0_18_64_3.669673618522336_1.7221734058804223.pt',64),\n",
    "    (\"Pinterest\",\"MLP\"):('LXR_Pinterest_MLP_0_5_16_10.059416809308486_0.705778173474644.pt',16),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type = output_type_dict[recommender_name] ### Can be single, multiple\n",
    "num_users = num_users_dict[data_name] \n",
    "num_items = num_items_dict[data_name] \n",
    "hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "\n",
    "recommender_path = recommender_path_dict[(data_name,recommender_name)]\n",
    "lxr_path = LXR_checkpoint_dict[(data_name,recommender_name)][0]\n",
    "lxr_dim = LXR_checkpoint_dict[(data_name,recommender_name)][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a6b87",
   "metadata": {},
   "source": [
    "## Data and baselines imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "with open(Path(files_path,f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f)\n",
    "train_array = train_data.to_numpy()\n",
    "test_array = test_data.to_numpy()\n",
    "items_array = np.eye(num_items)\n",
    "all_items_tensor = torch.Tensor(items_array).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dc3f3-5d7b-4e8d-bb16-8fb13920fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = static_test_data.iloc[:,:-2].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f572bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'jaccard_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "    jaccard_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'cosine_based_sim_{data_name}.pkl'), 'rb') as f:\n",
    "    cosine_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'tf_idf_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    tf_idf_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b9348-ad75-4e2f-8b31-75748a39255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'item_to_cluster_{recommender_name}_{data_name}.pkl'), 'rb') as f:\n",
    "    item_to_cluster = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34114bbf-0762-4a56-9f27-4a616af8a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(files_path, f'shap_values_{recommender_name}_{data_name}.pkl'), 'rb') as f:\n",
    "    shap_values= pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b741d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_items):\n",
    "    for j in range(i, num_items):\n",
    "        jaccard_dict[(j,i)]= jaccard_dict[(i,j)]\n",
    "        cosine_dict[(j,i)]= cosine_dict[(i,j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a826d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_array = np.zeros(len(pop_dict))\n",
    "for key, value in pop_dict.items():\n",
    "    pop_array[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_dict = {'device':device,\n",
    "          'num_items': num_items,\n",
    "          'pop_array':pop_array,\n",
    "          'all_items_tensor':all_items_tensor,\n",
    "          'static_test_data':static_test_data,\n",
    "          'items_array':items_array,\n",
    "          'output_type':output_type,\n",
    "          'recommender_name':recommender_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db85e",
   "metadata": {},
   "source": [
    "# Recommenders Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "\n",
    "VAE_config= {\n",
    "\"enc_dims\": hidden_dim,\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63243da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender():\n",
    "    if recommender_name=='MLP':\n",
    "        recommender = MLP(hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='VAE':\n",
    "        recommender = VAE(VAE_config, **kw_dict)\n",
    "    recommender_checkpoint = torch.load(Path(checkpoints_path, recommender_path))\n",
    "    recommender.load_state_dict(recommender_checkpoint)\n",
    "    recommender.eval()\n",
    "    for param in recommender.parameters():\n",
    "        param.requires_grad= False\n",
    "    return recommender\n",
    "    \n",
    "recommender = load_recommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94540246",
   "metadata": {},
   "source": [
    "# LXR definition and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48225b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainer(nn.Module):\n",
    "    def __init__(self, user_size, item_size, hidden_size):\n",
    "        super(Explainer, self).__init__()\n",
    "        \n",
    "        self.users_fc = nn.Linear(in_features = user_size, out_features=hidden_size).to(device)\n",
    "        self.items_fc = nn.Linear(in_features = item_size, out_features=hidden_size).to(device)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size*2, out_features=hidden_size).to(device),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features = hidden_size, out_features=user_size).to(device),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_output = self.users_fc(user_tensor.float())\n",
    "        item_output = self.items_fc(item_tensor.float())\n",
    "        combined_output = torch.cat((user_output, item_output), dim=-1)\n",
    "        expl_scores = self.bottleneck(combined_output).to(device)\n",
    "\n",
    "        return expl_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_explainer():\n",
    "    explainer = Explainer(num_items, num_items, lxr_dim)\n",
    "    lxr_checkpoint = torch.load(Path(checkpoints_path, lxr_path))\n",
    "    explainer.load_state_dict(lxr_checkpoint)\n",
    "    explainer.eval()\n",
    "    for param in explainer.parameters():\n",
    "        param.requires_grad= False\n",
    "    return explainer\n",
    "\n",
    "    \n",
    "explainer = load_explainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc79dc",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96048f75",
   "metadata": {},
   "source": [
    "# Baselines functions\n",
    "### Every function produces explanations for a designated baseline, resulting in a dictionary that maps items from the user's history to their explanation scores based on that baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097338a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../baselines') \n",
    "from ipynb.fs.defs.lime import *\n",
    "from ipynb.fs.defs.lime import *\n",
    "importlib.reload(ipynb.fs.defs.lime)\n",
    "from ipynb.fs.defs.lime import *\n",
    "\n",
    "lime = LimeBase(distance_to_proximity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25797d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User based similarities using Jaccard\n",
    "def find_jaccard_mask(x, item_id, user_based_Jaccard_sim):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_jaccard_dict = {}\n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            if (i,item_id) in user_based_Jaccard_sim:\n",
    "                item_jaccard_dict[i]=user_based_Jaccard_sim[(i,item_id)] # add Jaccard similarity between items\n",
    "            else:\n",
    "                item_jaccard_dict[i] = 0            \n",
    "\n",
    "    return item_jaccard_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950130f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine based similarities between users and items\n",
    "def find_cosine_mask(x, item_id, item_cosine):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    item_cosine_dict = {}\n",
    "    for i,j in enumerate(user_hist>0):\n",
    "        if j:\n",
    "            if (i,item_id) in item_cosine:\n",
    "                item_cosine_dict[i]=item_cosine[(i,item_id)]\n",
    "            else:\n",
    "                item_cosine_dict[i]=0\n",
    "\n",
    "    return item_cosine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lime_mask(x, item_id, min_pert, max_pert, num_of_perturbations, kernel_func, feature_selection, recommender, num_samples=10, method = 'POS', **kw_dict):\n",
    "    user_hist = x # remove the positive item we want to explain from the user history\n",
    "    user_hist[item_id] = 0\n",
    "    lime.kernel_fn = kernel_func\n",
    "    neighborhood_data, neighborhood_labels, distances, item_id = get_lime_args(user_hist, item_id, recommender, all_items_tensor, min_pert = min_pert, max_pert = max_pert, num_of_perturbations = num_of_perturbations, seed = item_id, **kw_dict)\n",
    "    if method=='POS':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection, pos_neg='POS')\n",
    "    if method=='NEG':\n",
    "        most_pop_items  = lime.explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, item_id, num_samples, feature_selection ,pos_neg='NEG')\n",
    "        \n",
    "    return most_pop_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fia_mask(user_tensor, item_tensor, item_id, recommender):\n",
    "    y_pred = recommender_run(user_tensor, recommender, item_tensor, item_id, **kw_dict).to(device)\n",
    "    items_fia = {}\n",
    "    user_hist = user_tensor.cpu().detach().numpy().astype(int)\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        if(user_hist[i] == 1):\n",
    "            user_hist[i] = 0\n",
    "            user_tensor = torch.FloatTensor(user_hist).to(device)\n",
    "            y_pred_without_item = recommender_run(user_tensor, recommender, item_tensor, item_id, 'single', **kw_dict).to(device)\n",
    "            infl_score = y_pred - y_pred_without_item\n",
    "            items_fia[i] = infl_score\n",
    "            user_hist[i] = 1\n",
    "\n",
    "    return items_fia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a35c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accent_mask(user_tensor, user_id, item_tensor, item_id, recommender_model, top_k):\n",
    "   \n",
    "    items_accent = defaultdict(float)\n",
    "    factor = top_k - 1\n",
    "    user_accent_hist = user_tensor.cpu().detach().numpy().astype(int)\n",
    "\n",
    "    #Get topk items\n",
    "    sorted_indices = list(get_top_k(user_tensor, user_tensor, recommender_model, **kw_dict).keys())\n",
    "    \n",
    "    if top_k == 1:\n",
    "        # When k=1, return the index of the first maximum value\n",
    "        top_k_indices = [sorted_indices[0]]\n",
    "    else:\n",
    "        top_k_indices = sorted_indices[:top_k]\n",
    "   \n",
    "\n",
    "    for iteration, item_k_id in enumerate(top_k_indices):\n",
    "\n",
    "        # Set topk items to 0 in the user's history\n",
    "        user_accent_hist[item_k_id] = 0\n",
    "        user_tensor = torch.FloatTensor(user_accent_hist).to(device)\n",
    "       \n",
    "        item_vector = items_array[item_k_id]\n",
    "        item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "              \n",
    "        # Check influence of the items in the history on this specific item in topk\n",
    "        fia_dict = find_fia_mask(user_tensor, item_tensor, item_k_id, recommender_model)\n",
    "         \n",
    "        # Sum up all differences between influence on top1 and other topk values\n",
    "        if not iteration:\n",
    "            for key in fia_dict.keys():\n",
    "                items_accent[key] *= factor\n",
    "        else:\n",
    "            for key in fia_dict.keys():\n",
    "                items_accent[key] -= fia_dict[key]\n",
    "       \n",
    "    for key in items_accent.keys():\n",
    "        items_accent[key] *= -1    \n",
    "\n",
    "    return items_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778184b0-ff25-4cbb-ad98-e506d0f83565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shapley_mask(user_tensor, user_id, model, shap_values, item_to_cluster):\n",
    "       \n",
    "    item_shap = {}\n",
    "    shapley_values = shap_values[shap_values[:, 0].astype(int) == user_id][:,1:]\n",
    "    user_vector = user_tensor.cpu().detach().numpy().astype(int)\n",
    "       \n",
    "    for i in np.where(user_vector.astype(int) == 1)[0]:\n",
    "        items_cluster = item_to_cluster[i]\n",
    "        item_shap[i] = shapley_values.T[int(items_cluster)][0]\n",
    " \n",
    "    return item_shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lxr_mask(x, item_tensor):\n",
    "    \n",
    "    user_hist = x\n",
    "    expl_scores = explainer(user_hist, item_tensor)\n",
    "    x_masked = user_hist*expl_scores\n",
    "    item_sim_dict = {}\n",
    "    for i,j in enumerate(x_masked>0):\n",
    "        if j:\n",
    "            item_sim_dict[i]=x_masked[i] \n",
    "        \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb094e",
   "metadata": {},
   "source": [
    "# Evaluation help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cdcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender_model, user_id = None, mask_type = None):\n",
    "    '''\n",
    "    This function invokes various explanation functions\n",
    "    and returns a dictionary of explanations, sorted by their scores.\n",
    "    '''\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "\n",
    "    if mask_type == 'lime':\n",
    "        POS_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 150, distance_to_proximity,'highest_weights', recommender_model, num_samples=user_hist_size, **kw_dict)\n",
    "        NEG_sim_items = find_lime_mask(user_vector, item_id, 50, 100, 150, distance_to_proximity,'highest_weights', recommender_model, num_samples=user_hist_size, method = 'NEG', **kw_dict)\n",
    "    else:\n",
    "        if mask_type == 'jaccard':\n",
    "            sim_items = find_jaccard_mask(user_tensor, item_id, jaccard_dict)\n",
    "        elif mask_type == 'cosine':\n",
    "            sim_items = find_cosine_mask(user_tensor, item_id, cosine_dict)\n",
    "        elif mask_type == 'shap':\n",
    "            sim_items = find_shapley_mask(user_tensor, user_id, recommender_model, shap_values, item_to_cluster)\n",
    "        elif mask_type == 'accent':\n",
    "            sim_items = find_accent_mask(user_tensor, user_id, item_tensor, item_id, recommender_model, 5)\n",
    "        elif mask_type == 'lxr':\n",
    "            sim_items = find_lxr_mask(user_tensor, item_tensor)\n",
    "        \n",
    "        POS_sim_items  = list(sorted(sim_items.items(), key=lambda item: item[1],reverse=True))[0:user_hist_size]\n",
    "        \n",
    "    return POS_sim_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be55f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_user_metrics(user_vector, user_tensor, item_id, item_tensor, num_of_bins, recommender_model, expl_dict, **kw_dict):\n",
    "    \n",
    "    '''\n",
    "    This function takes the explanation dictionary as input. \n",
    "    It iteratively removes the top 10% of items\n",
    "    that provide the most explanation from the user's history\n",
    "    and calculates the metric values for the resulting counterfactual user vector.\n",
    "    Ultimately, it returns all the metrics calculated at each step of masking.\n",
    "    '''\n",
    "    \n",
    "    POS_masked = user_tensor\n",
    "    NEG_masked = user_tensor\n",
    "    POS_masked[item_id]=0\n",
    "    NEG_masked[item_id]=0\n",
    "    user_hist_size = np.sum(user_vector)\n",
    "    \n",
    "    \n",
    "    bins=[0]+[len(x) for x in np.array_split(np.arange(user_hist_size), num_of_bins, axis=0)]\n",
    "    \n",
    "    POS_at_1 = [0]*(len(bins))\n",
    "    POS_at_5 = [0]*(len(bins))\n",
    "    POS_at_10=[0]*(len(bins))\n",
    "    POS_at_20=[0]*(len(bins))\n",
    "    POS_at_50=[0]*(len(bins))\n",
    "    POS_at_100=[0]*(len(bins))\n",
    "    \n",
    "    NEG_at_1 = [0]*(len(bins))\n",
    "    NEG_at_5 = [0]*(len(bins))\n",
    "    NEG_at_10 = [0]*(len(bins))\n",
    "    NEG_at_20 = [0]*(len(bins))\n",
    "    NEG_at_50 = [0]*(len(bins))\n",
    "    NEG_at_100 = [0]*(len(bins))\n",
    "    \n",
    "    DEL = [0]*(len(bins))\n",
    "    INS = [0]*(len(bins))\n",
    "    \n",
    "    NDCG = [0]*(len(bins))\n",
    "\n",
    "    \n",
    "    POS_sim_items = expl_dict\n",
    "    NEG_sim_items  = list(sorted(dict(POS_sim_items).items(), key=lambda item: item[1],reverse=False))\n",
    "    \n",
    "    total_items=0\n",
    "    for i in range(len(bins)):\n",
    "        total_items += bins[i]\n",
    "            \n",
    "        POS_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        \n",
    "        for j in POS_sim_items[:total_items]:\n",
    "            POS_masked[j[0]] = 1\n",
    "        POS_masked = user_tensor - POS_masked # remove the masked items from the user history\n",
    "\n",
    "        NEG_masked = torch.zeros_like(user_tensor, dtype=torch.float32, device=device)\n",
    "        for j in NEG_sim_items[:total_items]:\n",
    "            NEG_masked[j[0]] = 1\n",
    "        NEG_masked = user_tensor - NEG_masked # remove the masked items from the user history \n",
    "        \n",
    "        POS_ranked_list = get_top_k(POS_masked, user_tensor, recommender_model, **kw_dict)\n",
    "        \n",
    "        if item_id in list(POS_ranked_list.keys()):\n",
    "            POS_index = list(POS_ranked_list.keys()).index(item_id)+1\n",
    "        else:\n",
    "            POS_index = num_items\n",
    "        NEG_index = get_index_in_the_list(NEG_masked, user_tensor, item_id, recommender_model, **kw_dict)+1\n",
    "\n",
    "        # for pos:\n",
    "        POS_at_1[i] = 1 if POS_index <=1 else 0\n",
    "        POS_at_5[i] = 1 if POS_index <=5 else 0\n",
    "        POS_at_10[i] = 1 if POS_index <=10 else 0\n",
    "        POS_at_20[i] = 1 if POS_index <=20 else 0\n",
    "        POS_at_50[i] = 1 if POS_index <=50 else 0\n",
    "        POS_at_100[i] = 1 if POS_index <=100 else 0\n",
    "\n",
    "        # for neg:\n",
    "        NEG_at_1[i] = 1 if NEG_index <=1 else 0\n",
    "        NEG_at_5[i] = 1 if NEG_index <=5 else 0\n",
    "        NEG_at_10[i] = 1 if NEG_index <=10 else 0\n",
    "        NEG_at_20[i] = 1 if NEG_index <=20 else 0\n",
    "        NEG_at_50[i] = 1 if NEG_index <=50 else 0\n",
    "        NEG_at_100[i] = 1 if NEG_index <=100 else 0\n",
    "\n",
    "        # for del:\n",
    "        DEL[i] = float(recommender_run(POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "\n",
    "        # for ins:\n",
    "        INS[i] = float(recommender_run(user_tensor-POS_masked, recommender_model, item_tensor, item_id, **kw_dict).detach().cpu().numpy())\n",
    "\n",
    "        #for NDCG:\n",
    "        NDCG[i]= get_ndcg(list(POS_ranked_list.keys()),item_id, **kw_dict)\n",
    "        \n",
    "    res = [DEL, INS, NDCG, POS_at_1, POS_at_5, POS_at_10, POS_at_20, POS_at_50, POS_at_100,  NEG_at_1, NEG_at_5, NEG_at_10, NEG_at_20, NEG_at_50, NEG_at_100]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132628b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_dictionaries = True # if it is the first time generating the explanations\n",
    "\n",
    "if create_dictionaries:\n",
    "    import time\n",
    "    recommender.eval()\n",
    "    # Evaluate the model on the test set\n",
    "    \n",
    "    jaccard_expl_dict = {}\n",
    "    cosine_expl_dict = {}\n",
    "    lime_expl_dict = {}\n",
    "    accent_expl_dict = {}\n",
    "    shap_expl_dict = {}\n",
    "    lxr_expl_dict = {}    \n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_array.shape[0]):\n",
    "            if i%500 == 0:\n",
    "                print(i)\n",
    "            start_time = time.time()\n",
    "            user_vector = test_array[i]\n",
    "            user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "            user_id = int(test_data.index[i])\n",
    "\n",
    "            item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "            item_vector =  items_array[item_id]\n",
    "            item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "            user_vector[item_id] = 0\n",
    "            user_tensor[item_id] = 0\n",
    "\n",
    "            jaccard_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'jaccard')\n",
    "            cosine_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'cosine')\n",
    "            lime_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'lime')\n",
    "            accent_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'accent')\n",
    "            shap_expl_dict[user_id] = single_user_expl(user_vector, user_tensor,item_id, item_tensor, num_items, recommender, mask_type= 'shap',user_id = user_id)\n",
    "            lxr_expl_dict[user_id] = single_user_expl(user_vector, user_tensor, item_id, item_tensor, num_items, recommender, mask_type= 'lxr')\n",
    "            prev_time = time.time()\n",
    "            #print(\"User {}, total time: {:.2f}\".format(i,prev_time - start_time))\n",
    "\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_jaccard_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(jaccard_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_cosine_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(cosine_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lime_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(lime_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_accent_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(accent_expl_dict, handle) \n",
    "            \n",
    "        with open(Path(files_path,f'{recommender_name}_shap_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(shap_expl_dict, handle)\n",
    "\n",
    "        with open(Path(files_path,f'{recommender_name}_lxr_expl_dict.pkl'), 'wb') as handle:\n",
    "            pickle.dump(lxr_expl_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_expl_type(expl_name):\n",
    "    '''\n",
    "    This function aggregates explanations for all test users\n",
    "    and computes the average metric values across the entire test set.\n",
    "    '''\n",
    "    \n",
    "    print(f' ============ Start explaining {data_name} {recommender_name} by {expl_name} ============')\n",
    "    with open(Path(files_path,f'{recommender_name}_{expl_name}_expl_dict.pkl'), 'rb') as handle:\n",
    "        expl_dict = pickle.load(handle)\n",
    "    recommender.eval()\n",
    "    # Evaluate the model on the test set\n",
    "    num_of_bins = 11\n",
    "\n",
    "\n",
    "    users_DEL = np.zeros(num_of_bins)\n",
    "    users_INS = np.zeros(num_of_bins)\n",
    "    NDCG = np.zeros(num_of_bins)\n",
    "    POS_at_1 = np.zeros(num_of_bins)\n",
    "    POS_at_5 = np.zeros(num_of_bins)\n",
    "    POS_at_10 = np.zeros(num_of_bins)\n",
    "    POS_at_20 = np.zeros(num_of_bins)\n",
    "    POS_at_50 = np.zeros(num_of_bins)\n",
    "    POS_at_100 = np.zeros(num_of_bins)\n",
    "    NEG_at_1 = np.zeros(num_of_bins)\n",
    "    NEG_at_5 = np.zeros(num_of_bins)\n",
    "    NEG_at_10 = np.zeros(num_of_bins)\n",
    "    NEG_at_20 = np.zeros(num_of_bins)\n",
    "    NEG_at_50 = np.zeros(num_of_bins)\n",
    "    NEG_at_100 = np.zeros(num_of_bins)\n",
    "\n",
    "    num_of_bins=10\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(test_array.shape[0]):\n",
    "            start_time = time.time()\n",
    "            user_vector = test_array[i]\n",
    "            user_tensor = torch.FloatTensor(user_vector).to(device)\n",
    "            user_id = int(test_data.index[i])\n",
    "\n",
    "            item_id = int(get_user_recommended_item(user_tensor, recommender, **kw_dict).detach().cpu().numpy())\n",
    "            item_vector =  items_array[item_id]\n",
    "            item_tensor = torch.FloatTensor(item_vector).to(device)\n",
    "\n",
    "            user_vector[item_id] = 0\n",
    "            user_tensor[item_id] = 0\n",
    "\n",
    "            user_expl = expl_dict[user_id]\n",
    "\n",
    "            res = single_user_metrics(user_vector, user_tensor, item_id, item_tensor, num_of_bins, recommender, user_expl, **kw_dict)\n",
    "            users_DEL += res[0]\n",
    "            users_INS += res[1]\n",
    "            NDCG += res[2]\n",
    "            POS_at_1 += res[3]\n",
    "            POS_at_5 += res[4]\n",
    "            POS_at_10 += res[5]\n",
    "            POS_at_20 += res[6]\n",
    "            POS_at_50 += res[7]\n",
    "            POS_at_100 += res[8]\n",
    "            NEG_at_1 += res[9]\n",
    "            NEG_at_5 += res[10]\n",
    "            NEG_at_10 += res[11]\n",
    "            NEG_at_20 += res[12]\n",
    "            NEG_at_50 += res[13]\n",
    "            NEG_at_100 += res[14]\n",
    "\n",
    "            if i%500 == 0:\n",
    "                prev_time = time.time()\n",
    "                print(\"User {}, total time: {:.2f}\".format(i,prev_time - start_time))\n",
    "\n",
    "    a = i+1\n",
    "\n",
    "    print(f'users_DEL_{expl_name}: ', np.mean(users_DEL)/a)\n",
    "    print(f'users_INS_{expl_name}: ', np.mean(users_INS)/a)\n",
    "    print(f'NDCG_{expl_name}: ', np.mean(NDCG)/a)\n",
    "    print(f'POS_at_1_{expl_name}: ', np.mean(POS_at_1)/a)\n",
    "    print(f'POS_at_5_{expl_name}: ', np.mean(POS_at_5)/a)\n",
    "    print(f'POS_at_10_{expl_name}: ', np.mean(POS_at_10)/a)\n",
    "    print(f'POS_at_20_{expl_name}: ', np.mean(POS_at_20)/a)\n",
    "    print(f'POS_at_50_{expl_name}: ', np.mean(POS_at_50)/a)\n",
    "    print(f'POS_at_100_{expl_name}: ', np.mean(POS_at_100)/a)\n",
    "    print(f'NEG_at_1_{expl_name}: ', np.mean(NEG_at_1)/a)\n",
    "    print(f'NEG_at_5_{expl_name}: ', np.mean(NEG_at_5)/a)\n",
    "    print(f'NEG_at_10_{expl_name}: ', np.mean(NEG_at_10)/a)\n",
    "    print(f'NEG_at_20_{expl_name}: ', np.mean(NEG_at_20)/a)\n",
    "    print(f'NEG_at_50_{expl_name}: ', np.mean(NEG_at_50)/a)\n",
    "    print(f'NEG_at_100_{expl_name}: ', np.mean(NEG_at_100)/a)\n",
    "\n",
    "    \n",
    "    print(np.mean(users_DEL)/a , np.mean(users_INS)/a, np.mean(NDCG)/a , np.mean(POS_at_1)/a , np.mean(NEG_at_1)/a, np.mean(POS_at_5)/a , np.mean(NEG_at_5)/a, np.mean(POS_at_10)/a , np.mean(NEG_at_10)/a , np.mean(POS_at_20)/a , np.mean(NEG_at_20)/a, np.mean(POS_at_50)/a , np.mean(NEG_at_50)/a, np.mean(POS_at_100)/a , np.mean(NEG_at_100)/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d66300",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_names_list = ['lxr'] # specify the names of the baselines for which you wish to calculate the metrics values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c4d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for expl_name in expl_names_list:\n",
    "    eval_one_expl_type(expl_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
