{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9956ce",
   "metadata": {},
   "source": [
    "### This notebook presents the architectures of the two recommendation systems tested within this framework\n",
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6583e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import optuna\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd66af",
   "metadata": {},
   "source": [
    "# 2. MLP recommender Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, **kw):\n",
    "        super(MLP, self).__init__()\n",
    "        user_size = kw['num_items']\n",
    "        item_size = kw['num_items']\n",
    "        self.device = kw['device']\n",
    "        self.users_fc = nn.Linear(user_size, hidden_size, bias = True).to(self.device)\n",
    "        self.items_fc = nn.Linear(item_size, hidden_size, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_vec = self.users_fc(user_tensor.to(self.device))\n",
    "        item_vec = self.items_fc(item_tensor.to(self.device))\n",
    "        output = torch.matmul(user_vec, item_vec.T).to(self.device)\n",
    "        return self.sigmoid(output).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c694d",
   "metadata": {},
   "source": [
    "# 3. VAE recommender Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, model_conf, **kw):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = kw['device'] \n",
    "        num_items = kw['num_items'] \n",
    "        self.num_items = num_items\n",
    "        self.enc_dims = [self.num_items] + model_conf['enc_dims']\n",
    "        self.dec_dims = self.enc_dims[::-1]\n",
    "        self.dims = self.enc_dims + self.dec_dims[1:]\n",
    "        self.dropout = model_conf['dropout']\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.total_anneal_steps = model_conf['total_anneal_steps']\n",
    "        self.anneal_cap = model_conf['anneal_cap']\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.anneal = 0.\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.enc_dims[:-1], self.enc_dims[1:])):\n",
    "            if i == len(self.enc_dims[:-1]) - 1:\n",
    "                d_out *= 2\n",
    "            self.encoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.enc_dims[:-1]) - 1:\n",
    "                self.encoder.append(nn.ReLU())\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dec_dims[:-1], self.dec_dims[1:])):\n",
    "            self.decoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.dec_dims[:-1]) - 1:\n",
    "                self.decoder.append(nn.ReLU())\n",
    "                \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, rating_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param rating_matrix: rating matrix\n",
    "        \"\"\"\n",
    "        # encoder\n",
    "        if len(rating_matrix.shape) == 1:\n",
    "            rating_matrix = torch.unsqueeze(rating_matrix, 0)\n",
    "        h = F.dropout(F.normalize(rating_matrix, dim=-1), p=self.dropout, training=self.training)\n",
    "        for layer in self.encoder:\n",
    "            h = layer(h)\n",
    "\n",
    "        # sample\n",
    "        mu_q = h[:, :self.enc_dims[-1]]\n",
    "        logvar_q = h[:, self.enc_dims[-1]:]  # log sigmod^2  batch x 200\n",
    "        std_q = torch.exp(0.5 * logvar_q)  # sigmod batch x 200\n",
    "        \n",
    "        epsilon = torch.zeros_like(std_q).normal_(mean=0, std=0.01)\n",
    "        sampled_z = mu_q + self.training * epsilon * std_q\n",
    "\n",
    "        output = sampled_z\n",
    "        for layer in self.decoder:\n",
    "            output = layer(output)\n",
    "\n",
    "        if self.training:\n",
    "            kl_loss = ((0.5 * (-logvar_q + torch.exp(logvar_q) + torch.pow(mu_q, 2) - 1)).sum(1)).mean()\n",
    "            return output, kl_loss\n",
    "        else:\n",
    "            return self.softmax(output)   \n",
    "        \n",
    "    def train_one_epoch(self, dataset, optimizer, batch_size, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Train model for one epoch\n",
    "        :param dataset: given data\n",
    "        :param optimizer: choice of optimizer\n",
    "        :param batch_size: batch size\n",
    "        :return: model loss\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        train_matrix = dataset\n",
    "\n",
    "        num_training = train_matrix.shape[0]\n",
    "        num_batches = int(np.ceil(num_training / batch_size))\n",
    "        perm = np.random.permutation(num_training)\n",
    "\n",
    "        loss = 0.0\n",
    "        for b in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (b + 1) * batch_size >= num_training:\n",
    "                batch_idx = perm[b * batch_size:]\n",
    "            else:\n",
    "                batch_idx = perm[b * batch_size: (b + 1) * batch_size]\n",
    "            batch_matrix = torch.FloatTensor(train_matrix[batch_idx]).to(self.device)\n",
    "\n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.anneal_cap, 1. * self.update_count / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.anneal_cap\n",
    "\n",
    "            pred_matrix, kl_loss = self.forward(batch_matrix)\n",
    "\n",
    "            # cross_entropy\n",
    "            total_ce = -(F.log_softmax(pred_matrix, 1) * batch_matrix)\n",
    "            ce_hist = total_ce[:,:self.num_items].sum(1).mean()\n",
    "            ce_demo = total_ce[:,self.num_items:].sum(1).mean()\n",
    "            ce_loss = ce_hist+alpha*ce_demo\n",
    "\n",
    "            batch_loss = ce_loss + kl_loss * self.anneal\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            self.update_count += 1\n",
    "\n",
    "            loss += batch_loss\n",
    "            if b % 200 == 0:\n",
    "                print('(%3d / %3d) loss = %.4f' % (b, num_batches, batch_loss))\n",
    "        return loss\n",
    "\n",
    "    def predict(self, eval_users, test_batch_size):\n",
    "        \"\"\"\n",
    "        Predict the model on test set\n",
    "        :param eval_users: evaluation (test) user\n",
    "        :param eval_pos: position of the evaluated (test) item\n",
    "        :param test_batch_size: batch size for test set\n",
    "        :return: predictions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            input_matrix = torch.Tensor(eval_users).to(self.device)\n",
    "            preds = np.zeros_like(input_matrix.cpu())\n",
    "\n",
    "            num_data = input_matrix.shape[0]\n",
    "            num_batches = int(np.ceil(num_data / test_batch_size))\n",
    "            perm = list(range(num_data))\n",
    "            for b in range(num_batches):\n",
    "                if (b + 1) * test_batch_size >= num_data:\n",
    "                    batch_idx = perm[b * test_batch_size:]\n",
    "                else:\n",
    "                    batch_idx = perm[b * test_batch_size: (b + 1) * test_batch_size]\n",
    "                    \n",
    "                test_batch_matrix = input_matrix[batch_idx]\n",
    "                batch_pred_matrix = self.forward(test_batch_matrix)\n",
    "                batch_pred_matrix.masked_fill(test_batch_matrix.bool(), float('-inf'))\n",
    "                preds[batch_idx] = batch_pred_matrix.detach().cpu().numpy()\n",
    "        return preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
