{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3d15ab",
   "metadata": {},
   "source": [
    "### This notebook documents the data preprocessing processes applied to the three datasets: \"MovieLens 1 Million\", \"Yahoo\", and \"Pinterest\".\n",
    "\n",
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160299a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, Yahoo, Pinterest\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir.parent, DP_DIR)\n",
    "min_num_of_items_per_user = 2\n",
    "min_num_of_users_per_item = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML1M data\n",
    "if data_name == \"ML1M\":\n",
    "    data = pd.read_csv(Path(files_path, \"ratings.dat\"), sep=\"::\", engine=\"python\",\n",
    "                       names=[\"user_id_original\", \"item_id_original\", \"rating\", \"timestamp\"])\n",
    "    \n",
    "# Load Yahoo data\n",
    "elif data_name == \"Yahoo\":\n",
    "    data = pd.read_csv(Path(files_path, \"Yahoo_ratings.csv\"), names=[\"user_id_original\", \"item_id_original\", \"rating\"])\n",
    "\n",
    "# Load Pinterest data\n",
    "elif data_name == \"Pinterest\":\n",
    "    data = pd.read_csv(Path(files_path, \"pinterest_data.csv\"), names=[\"user_id_original\", \"item_id_original\", \"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3855c3",
   "metadata": {},
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118301a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ratings to binary values (1 if rating exists, 0 otherwise). \n",
    "# Keep only ratings over 70/100.\n",
    "\n",
    "if data_name=='Yahoo':\n",
    "    data[\"rating\"] = data[\"rating\"].apply(lambda x: 0 if x == 255 else x) # for Yahoo only\n",
    "    data[\"rating\"] = data[\"rating\"].apply(lambda x: 1 if x > 70 else 0)\n",
    "elif data_name=='ML1M' or data_name==\"ML1M_demographic\":\n",
    "    data[\"rating\"] = data[\"rating\"].apply(lambda x: 1 if x > 3.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a59772",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['rating']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08013e",
   "metadata": {},
   "source": [
    "### recursively delete users and items with too few interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_1 = 1\n",
    "num_rows_2 = 2\n",
    "\n",
    "while num_rows_1 != num_rows_2:\n",
    "    # save only users with min_num_of_items_per_user items or more\n",
    "    user_counts = data.groupby(['user_id_original'])['item_id_original'].nunique().reset_index(name='item_count')\n",
    "    filtered_users = user_counts[user_counts['item_count'] >= min_num_of_items_per_user]['user_id_original']\n",
    "    data = data[data['user_id_original'].isin(filtered_users)].reset_index(drop=True)\n",
    "    num_rows_1 = data.shape[0]\n",
    "    \n",
    "    # save only items with min_num_of_users_per_item users or more\n",
    "    item_counts = data.groupby(['item_id_original'])['user_id_original'].nunique().reset_index(name='user_count')\n",
    "    filtered_items = item_counts[item_counts['user_count'] >= min_num_of_users_per_item]['item_id_original']\n",
    "    data = data[data['item_id_original'].isin(filtered_items)].reset_index(drop=True)\n",
    "    num_rows_2 = data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target values\n",
    "item_encoder = LabelEncoder()\n",
    "user_encoder = LabelEncoder()\n",
    "user_encoder.fit(data.user_id_original)\n",
    "item_encoder.fit(data.item_id_original)\n",
    "\n",
    "data[\"user_id\"] = user_encoder.transform(data.user_id_original)\n",
    "data[\"item_id\"] = item_encoder.transform(data.item_id_original)\n",
    "\n",
    "# Get the number of users and items in the dataset\n",
    "num_users = data.user_id.unique().shape[0]\n",
    "num_items = data.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num_items = ', num_items, ' num_users = ', num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aad07",
   "metadata": {},
   "source": [
    "##  transform the data to encoding representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data to encoding representation\n",
    "user_group = data[[\"user_id\",\"item_id\"]].groupby(data.user_id)\n",
    "\n",
    "users_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(user_group.groups.keys()),\n",
    "        \"item_ids\": list(user_group.item_id.apply(list)),\n",
    "    }    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad57051",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "user_one_hot = pd.DataFrame(mlb.fit_transform(users_data[\"item_ids\"]),columns=mlb.classes_, index=users_data[\"item_ids\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_one_hot[\"user_id\"]=users_data[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(user_one_hot.iloc[:,:-1], user_one_hot.iloc[:,-1], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9c273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index = np.arange(X_train.shape[0], num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(Path(files_path, f'test_data_{data_name}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1628df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(Path(files_path, f'train_data_{data_name}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d8d21",
   "metadata": {},
   "source": [
    "# 3. Create dictionaries for baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128b93",
   "metadata": {},
   "source": [
    "### Jaccard dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = X_train.to_numpy() #np array of one hot, shape (|U_train|,|I|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcacab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jaccard_dict = {}\n",
    "for i in range(num_features):\n",
    "    for j in range(i, num_features):\n",
    "        intersection = (data_array[:,i]*data_array[:,j]).sum()\n",
    "        union = np.count_nonzero(data_array[:,i]+data_array[:,j])\n",
    "        if union == 0:\n",
    "            jaccard_dict[(i,j)]=0\n",
    "        else:\n",
    "            jaccard_dict[(i,j)]=(intersection/union).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e714a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(files_path, f'jaccard_based_sim_{data_name}.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(jaccard_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ba53b",
   "metadata": {},
   "source": [
    "### Cosine dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bbd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_items = cosine_similarity(X_train.T).astype('float32')\n",
    "cosine_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd194e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_items_dict = {}\n",
    "\n",
    "# Loop through the rows and columns of the ndarray and add each element to the dictionary\n",
    "for i in range(cosine_items.shape[0]):\n",
    "    for j in range(i,cosine_items.shape[1]):\n",
    "        cosine_items_dict[(i, j)] = cosine_items[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bfc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(files_path, f'cosine_based_sim_{data_name}.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(cosine_items_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d2059",
   "metadata": {},
   "source": [
    "### Popularity dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_array = (X_train.sum(axis=0)/X_train.sum(axis=0).max()).astype('float32') \n",
    "pop_dict = {}\n",
    "\n",
    "for i in range(num_items):\n",
    "    pop_dict[i]=pop_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(files_path, f'pop_dict_{data_name}.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(pop_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3878fb9",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = pd.concat([X_train, X_test], axis=0).to_numpy() #np array of one hot, shape (|U|,|I|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_count = user_one_hot.iloc[:,:-1].sum(axis=1) # numer of items in user's history, shape = |U|\n",
    "\n",
    "n_appearance = user_one_hot.iloc[:,:-1].sum(axis=0) # number of appearances of item in user histories, shape = |I|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b33e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dict = defaultdict(dict)\n",
    "for u in range(num_users):\n",
    "    for i in range(num_items):\n",
    "        if data_array[u,i] == 1:\n",
    "            tf = 1/w_count[u]\n",
    "            idf = np.log10(num_users/n_appearance[i])\n",
    "            tf_idf_dict[u][i] = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(files_path, f'tf_idf_dict_{data_name}.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(tf_idf_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
