{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18831b05",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160299a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import ipynb\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56467a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\"\n",
    "}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155\n",
    "}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, Yahoo, Pinterest\n",
    "recommender_name = \"MLP\" ## Can be MLP, VAE, MLP_model, GMF_model, NCF\n",
    "\n",
    "\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd())\n",
    "files_path = Path(export_dir.parent, DP_DIR)\n",
    "checkpoints_path = Path(export_dir, \"checkpoints\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output_type = output_type_dict[recommender_name] ### Can be single, multiple\n",
    "num_users = num_users_dict[data_name] \n",
    "num_items = num_items_dict[data_name] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a6b87",
   "metadata": {},
   "source": [
    "## Data imports and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "with open(Path(files_path,f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f)\n",
    "train_array = train_data.to_numpy()\n",
    "test_array = test_data.to_numpy()\n",
    "items_array = np.eye(num_items)\n",
    "all_items_tensor = torch.Tensor(items_array).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29108508",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(static_test_data.shape[0]):\n",
    "    static_test_data.iloc[row, static_test_data.iloc[row,-2]]=0\n",
    "test_array = static_test_data.iloc[:,:-2].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdd7e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pop_array = np.zeros(len(pop_dict))\n",
    "for key, value in pop_dict.items():\n",
    "    pop_array[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db85e",
   "metadata": {},
   "source": [
    "# Recommenders import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd36660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc79dc",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e23de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_dict = {'device':device,\n",
    "          'num_items': num_items,\n",
    "          'pop_array':pop_array,\n",
    "          'all_items_tensor':all_items_tensor,\n",
    "          'static_test_data':static_test_data,\n",
    "          'items_array':items_array,\n",
    "          'output_type':output_type,\n",
    "          'recommender_name':recommender_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771d610",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b818020",
   "metadata": {},
   "source": [
    "## MLP Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "HR10_dict = {}\n",
    "\n",
    "def MLP_objective(trial):\n",
    "    \n",
    "    lr = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
    "    beta = trial.suggest_float('beta', 0, 4) # hyperparameter that weights the different loss terms\n",
    "    epochs = 10\n",
    "    model = MLP(hidden_dim, **kw_dict)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    hr10 = []\n",
    "    \n",
    "    print(f'======================== new run - {recommender_name} ========================')\n",
    "    logger.info(f'======================== new run - {recommender_name} ========================')\n",
    "    \n",
    "    num_training = train_data.shape[0]\n",
    "    num_batches = int(np.ceil(num_training / batch_size))\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_matrix = sample_indices(train_data.copy(), **kw_dict)\n",
    "        perm = np.random.permutation(num_training)\n",
    "        loss = []\n",
    "        train_pos_loss=[]\n",
    "        train_neg_loss=[]\n",
    "        if epoch!=0 and epoch%10 == 0: # decrease the learning rate every 10 epochs\n",
    "            lr = 0.1*lr\n",
    "            optimizer.lr = lr\n",
    "        \n",
    "        for b in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            if (b + 1) * batch_size >= num_training:\n",
    "                batch_idx = perm[b * batch_size:]\n",
    "            else:\n",
    "                batch_idx = perm[b * batch_size: (b + 1) * batch_size]    \n",
    "            batch_matrix = torch.FloatTensor(train_matrix[batch_idx,:-2]).to(device)\n",
    "\n",
    "            batch_pos_idx = train_matrix[batch_idx,-2]\n",
    "            batch_neg_idx = train_matrix[batch_idx,-1]\n",
    "            \n",
    "            batch_pos_items = torch.Tensor(items_array[batch_pos_idx]).to(device)\n",
    "            batch_neg_items = torch.Tensor(items_array[batch_neg_idx]).to(device)\n",
    "            \n",
    "            pos_output = torch.diagonal(model(batch_matrix, batch_pos_items))\n",
    "            neg_output = torch.diagonal(model(batch_matrix, batch_neg_items))\n",
    "            \n",
    "            # MSE loss\n",
    "            pos_loss = torch.mean((torch.ones_like(pos_output)-pos_output)**2)\n",
    "            neg_loss = torch.mean((neg_output)**2)\n",
    "            \n",
    "            batch_loss = pos_loss + beta*neg_loss\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss.append(batch_loss.item())\n",
    "            train_pos_loss.append(pos_loss.item())\n",
    "            train_neg_loss.append(neg_loss.item())\n",
    "            \n",
    "        print(f'train pos_loss = {np.mean(train_pos_loss)}, neg_loss = {np.mean(train_neg_loss)}')    \n",
    "        train_losses.append(np.mean(loss))\n",
    "        torch.save(model.state_dict(), Path(checkpoints_path, f'MLP_{data_name}_{round(lr,4)}_{batch_size}_{trial.number}_{epoch}.pt'))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_matrix = np.array(static_test_data)\n",
    "        test_tensor = torch.Tensor(test_matrix[:,:-2]).to(device)\n",
    "        \n",
    "        test_pos = test_matrix[:,-2]\n",
    "        test_neg = test_matrix[:,-1]\n",
    "        \n",
    "        row_indices = np.arange(test_matrix.shape[0])\n",
    "        test_tensor[row_indices,test_pos] = 0\n",
    "        \n",
    "        pos_items = torch.Tensor(items_array[test_pos]).to(device)\n",
    "        neg_items = torch.Tensor(items_array[test_neg]).to(device)\n",
    "        \n",
    "        pos_output = torch.diagonal(model(test_tensor, pos_items).to(device))\n",
    "        neg_output = torch.diagonal(model(test_tensor, neg_items).to(device))\n",
    "        \n",
    "        pos_loss = torch.mean((torch.ones_like(pos_output)-pos_output)**2)\n",
    "        neg_loss = torch.mean((neg_output)**2)\n",
    "        print(f'test pos_loss = {pos_loss}, neg_loss = {neg_loss}')\n",
    "        \n",
    "        hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model, **kw_dict)\n",
    "        hr10.append(hit_rate_at_10) # metric for monitoring\n",
    "        print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)\n",
    "        \n",
    "        test_losses.append(-hit_rate_at_10)\n",
    "        if epoch>5: # early stop if the HR@10 decreases for 4 epochs in a row\n",
    "            if test_losses[-2]<=test_losses[-1] and test_losses[-3]<=test_losses[-2] and test_losses[-4]<=test_losses[-3]:\n",
    "                logger.info(f'Early stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "                train_losses_dict[trial.number] = train_losses\n",
    "                test_losses_dict[trial.number] = test_losses\n",
    "                HR10_dict[trial.number] = hr10\n",
    "                return max(hr10)\n",
    "            \n",
    "    logger.info(f'Stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "    train_losses_dict[trial.number] = train_losses\n",
    "    test_losses_dict[trial.number] = test_losses\n",
    "    HR10_dict[trial.number] = hr10\n",
    "    return max(hr10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc5616",
   "metadata": {},
   "source": [
    "## VAE Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "HR10_dict = {}\n",
    "\n",
    "VAE_config= {\n",
    "\"enc_dims\": [256,64],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000\n",
    "}\n",
    "\n",
    "def VAE_objective(trial):\n",
    "    \n",
    "    lr = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64,128,256])\n",
    "    epochs = 20\n",
    "    model = VAE(VAE_config ,**kw_dict)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    hr10 = []\n",
    "    print('======================== new run ========================')\n",
    "    logger.info('======================== new run ========================')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epoch!=0 and epoch%10 == 0:\n",
    "            lr = 0.1*lr\n",
    "            optimizer.lr = lr\n",
    "        loss = model.train_one_epoch(train_array, optimizer, batch_size)\n",
    "        train_losses.append(loss)\n",
    "        torch.save(model.state_dict(), Path(checkpoints_path, f'VAE_{data_name}_{trial.number}_{epoch}_{round(lr,4)}_{batch_size}.pt'))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_matrix = static_test_data.to_numpy()\n",
    "        test_tensor = torch.Tensor(test_matrix[:,:-2]).to(device)\n",
    "        test_pos = test_array[:,-2]\n",
    "        test_neg = test_array[:,-1]\n",
    "        row_indices = np.arange(test_matrix.shape[0])\n",
    "        test_tensor[row_indices,test_pos] = 0\n",
    "        output = model(test_tensor).to(device)\n",
    "        pos_loss = -output[row_indices,test_pos].mean()\n",
    "        neg_loss = output[row_indices,test_neg].mean()\n",
    "        print(f'pos_loss = {pos_loss}, neg_loss = {neg_loss}')\n",
    "        \n",
    "        hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model, **kw_dict)\n",
    "        hr10.append(hit_rate_at_10)\n",
    "        print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)\n",
    "        \n",
    "        test_losses.append(pos_loss.item())\n",
    "        if epoch>5:\n",
    "            if test_losses[-2]<test_losses[-1] and test_losses[-3]<test_losses[-2] and test_losses[-4]<test_losses[-3]:\n",
    "                logger.info(f'Early stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "                train_losses_dict[trial.number] = train_losses\n",
    "                test_losses_dict[trial.number] = test_losses\n",
    "                HR10_dict[trial.number] = hr10\n",
    "                return max(hr10)\n",
    "    \n",
    "    logger.info(f'Stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "    train_losses_dict[trial.number] = train_losses\n",
    "    test_losses_dict[trial.number] = test_losses\n",
    "    HR10_dict[trial.number] = hr10\n",
    "    return max(hr10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b77e2",
   "metadata": {},
   "source": [
    "### Save logs in text file, optimize using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b4d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "logger.setLevel(logging.INFO)  # Setup the root logger.\n",
    "logger.addHandler(logging.FileHandler(f\"{recommender_name}_{data_name}_Optuna.log\", mode=\"w\"))\n",
    "\n",
    "optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
    "optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "logger.info(\"Start optimization.\")\n",
    "\n",
    "if recommender_name == 'MLP':\n",
    "    study.optimize(MLP_objective, n_trials=20) \n",
    "elif recommender_name == 'VAE':\n",
    "    study.optimize(VAE_objective, n_trials=20) \n",
    "\n",
    "with open(f\"{recommender_name}_{data_name}_Optuna.log\") as f:\n",
    "    assert f.readline().startswith(\"A new study created\")\n",
    "    assert f.readline() == \"Start optimization.\\n\"\n",
    "    \n",
    "    \n",
    "# Print best hyperparameters and corresponding metric value\n",
    "print(\"Best hyperparameters: {}\".format(study.best_params))\n",
    "print(\"Best metric value: {}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d135397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in HR10_dict.keys():\n",
    "    print(run, np.argmax(HR10_dict[run]), max(HR10_dict[run]))\n",
    "    plt.plot(HR10_dict[run])\n",
    "plt.legend(HR10_dict.keys(), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82576ff3",
   "metadata": {},
   "source": [
    "# Evaluations\n",
    "### Load the trained recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65016d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfe9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP1_ML1M_0.0076_256_7.pt\"),\n",
    "\n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_12_18_0.0001_256.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\"),\n",
    "    \n",
    "}\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): None,\n",
    "    (\"ML1M\",\"MLP\"): 32,\n",
    "\n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = hidden_dim_dict[(data_name,recommender_name)]\n",
    "recommender_path = recommender_path_dict[(data_name,recommender_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender():\n",
    "    if recommender_name=='MLP':\n",
    "        recommender = MLP(hidden_dim, **kw_dict)\n",
    "    elif recommender_name=='VAE':\n",
    "        recommender = VAE(VAE_config, **kw_dict)\n",
    "    recommender_checkpoint = torch.load(Path(checkpoints_path, recommender_path))\n",
    "    recommender.load_state_dict(recommender_checkpoint)\n",
    "    recommender.eval()\n",
    "    for param in recommender.parameters():\n",
    "        param.requires_grad= False\n",
    "    return recommender\n",
    "    \n",
    "model = load_recommender()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34153c1f",
   "metadata": {},
   "source": [
    "### Plot the distribution of top recommended item accross all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ceeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of top recommended item accross all users\n",
    "topk_train = {}\n",
    "for i in range(len(train_array)):\n",
    "    vec = train_array[i]\n",
    "    tens = torch.Tensor(vec).to(device)\n",
    "    topk_train[i] = int(get_user_recommended_item(tens, model, **kw_dict).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934526cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(topk_train.values(), bins=1000)\n",
    "plt.plot(np.array(list(pop_dict.keys())), np.array(list(pop_dict.values()))*100, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_test = {}\n",
    "for i in range(len(test_array)):\n",
    "    vec = test_array[i]\n",
    "    tens = torch.Tensor(vec).to(device)\n",
    "    topk_test[i] = int(get_user_recommended_item(tens, model, **kw_dict).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d194ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(topk_test.values(), bins=400)\n",
    "plt.plot(np.array(list(pop_dict.keys())), np.array(list(pop_dict.values()))*50, alpha=0.2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5774b8",
   "metadata": {},
   "source": [
    "### Calculate recommender's HR@10, HR@50, HR@100, MRR and MPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9637160",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model, **kw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ae07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
